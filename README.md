# 日常收集的概念点滴

## 2025/10/2
1. 单个使用 Sigmoid 激活函数的神经元，其计算过程和逻辑回归的数学模型是完全一样的。
2. 神经网络的核心工作方式，就是用学到的经验（权重模板）去“卷积”新的数据，从而提取信息、做出判断。
3. 用两个神经元相减来构造一个“凸起”函数。比如，`neuron1 - neuron2` = $\sigma(w_1x+b_1) - \sigma(w_2x+b_2)$。许多神经元 ≈ 函数的“积木块”。隐藏层的神经元数量越多，拥有的积木块就越多、越精细，就能把目标函数拟合得越平滑、越精确。
4. 通过一层层隐藏层，将原本线性不可分的点，移动到新的空间位置，让它们变得线性可分。
5. 神经网络的成功，是其强大的表达能力、极致的计算效率（尤其是在并行硬件上）和可扩展性之间达成的一种近乎完美的平衡。
    - 是否存在“更优秀”的单元？
        - 核方法 (Kernel Methods)，如支持向量机 (SVM)
        - 高斯过程 (Gaussian Processes)
        - 样条回归 (Spline Regression) / 决策树 (Decision Trees)
6. 动态结构 (Dynamic Architecture)
    - 神经结构搜索 (Neural Architecture Search, NAS)
    - 动态神经网络 (Dynamic Neural Networks)
7. 结构自组织的数学原理
    - 动态图 (Dynamic Graphs) 的数学框架
    - 发育过程 (Developmental Processes) 和自组织原理 (Self-Organization)
    - 预测编码 (Predictive Coding) 理论
8. 从某一层的角度看（比如第 $L$ 层），它的输入来自于前一层（第 $L-1$ 层）。当我们在一次迭代中更新了 $L-1$ 层的权重后，下一次前向传播时，$L-1$ 层的输出分布就变了。这意味着第 $L$ 层突然要面对一个和它上次训练时略有不同的“新问题”。
    - 这个问题在学术上被称为“内部协变量偏移”（Internal Covariate Shift）。著名的**批量归一化（Batch Normalization）**技术，其最初的动机就是为了缓解这个问题，通过在每一层之后对激活值进行重新标准化，来稳定每一层输入的分布。
