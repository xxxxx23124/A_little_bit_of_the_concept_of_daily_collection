# 日常收集的概念点滴

## 2025/10/4
1. 神经网络的“卷积”视角
    1. 神经网络的核心是使用学到的“权重模板”（通过训练优化得到的参数）对输入数据进行加权组合或过滤。
        - 在CNN中，这是局部卷积：一个小的固定内核（e.g., 3x3权重矩阵）滑动扫描输入，提取局部特征。
        - 在全连接层（Fully Connected, FC）中，这是全局“卷积”：一个大的权重矩阵一次性连接所有输入到所有输出，等价于一个覆盖整个输入的“全局内核”，进行加权求和。

        输出 = 权重模板 ⊗ 输入（这里⊗泛指加权操作，如矩阵乘法或卷积）。

    2. 注意力机制作为动态“卷积”
        注意力机制可以视为一种自适应全局卷积：它不使用固定权重模板，而是先从输入中“学习”出一个动态的权重矩阵（注意力分数矩阵），然后用这个矩阵对输入进行加权“卷积”（实际上是矩阵乘法或加权求和）。
            学到的经验部分：注意力机制依赖于三个投影矩阵（W_Q, W_K, W_V），这些是训练过程中优化的“权重模板”。它们类似于全连接层的权重，用于将输入X投影成Query (Q = X W_Q)、Key (K = X W_K)和Value (V = X W_V)。这些投影矩阵捕捉了模型的“经验”，指导如何从输入中提取查询、键和值。
            动态生成权重模板：注意力分数 = softmax(Q K^T / sqrt(d_k))，这是一个N x N的矩阵（N是序列长度），表示每个位置对其他位置的关注度。这个分数矩阵就是动态的“卷积内核”——它不是固定的，而是基于当前输入的自相关性计算出来的（Q和K的点积捕捉相似度）。
            “卷积”新数据：最终输出 = 注意力分数 ⊗ V。这类似于用动态内核对值V进行全局加权求和，每个输出元素都是输入所有元素的加权组合，但权重是输入驱动的。
        与全连接层的比较：
        全连接层：输出 = W X（W是固定全局权重矩阵）。
        注意力：输出 ≈ (softmax(Q K^T)) V，其中Q/K/V都从X投影而来。这相当于一个“输入依赖”的全局权重矩阵，允许模型根据上下文调整连接强度（e.g., 在NLP中，关注相关词而忽略无关词）。

    3. 数学解读（简化版）
        - 假设输入X ∈ ℝ^{N x d}（N序列长，d维度）。
         - Q = X W_Q, K = X W_K, V = X W_V（W_* 是学到的d x d_k权重模板）。
         - 注意力分数A = softmax( (Q K^T) / √d_k ) ∈ ℝ^{N x N}（动态模板，行归一化）。
         - 输出Z = A V（“卷积”：每个输出行是V的加权和）。
        - 这可以看作Z_i = ∑_j A_{i,j} V_j，其中A_{i,j}是位置i对j的“卷积权重”，动态计算而非固定。
    4. 那能不能用 六个小神经网络 动态生成 六个小向量 构成（如外积、拼接后重塑等） W_Q, W_K, W_V 的参数矩阵呢？
        - 动态网络（Dynamic Networks）或超网络（Hypernetworks）
            - 主网络（Main Network）：这是我们熟悉的注意力机制，它需要 W_Q, W_K, W_V 这三个参数矩阵。
            - 超网络（Hypernetwork）：用“六个小神经网络”来动态生成 W_Q, W_K, W_V 的参数。这个超网络的输入可以是全局上下文、任务指令，甚至是当前输入数据本身的一部分。
            - 生成过程：这六个小网络各自生成一个小向量。这些向量通过某种组合方式（如外积、拼接后重塑等）构成最终的 W_Q, W_K, W_V 矩阵。
            - 这个模型可以表示为：
                * θ_W = f_hyper(c)
                * Output = Attention(X, θ_W)
    