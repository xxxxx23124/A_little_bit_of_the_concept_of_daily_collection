1. 负熵损失 (Negative Entropy Loss) 
   原理: 熵衡量的是概率分布的“不确定性”或“均匀性”。最大化熵（等价于最小化负熵）会促使softmax的输出 coeffs 趋向于一个均匀分布（例如，对于两个专家，趋向于 [0.5, 0.5]）。 
2. 负载均衡损失 (Load Balancing Loss) - 原始MoE的方式 
   原理: 它不关心单个样本的路由决策，而是关心整个批次（batch）中每个专家的总负载。  
   首先，计算每个专家在整个批次中被分配到的总权重（importance）。  
   然后，计算这些 importance 值的变异系数（coefficient of variation）的平方，并将其作为损失。目标是让所有专家的总负载尽可能接近。 
3. Router Z-Loss (来自Switch Transformer): 
   原理: 它不直接操作softmax后的概率，而是操作softmax前的logits。它惩罚logits的量级（magnitude）。 
   z_loss = torch.logsumexp(logits, dim=-1).pow(2).mean() 
   logsumexp约等于max(logits)。所以这个损失本质上是在说：“不要让任何一个logit变得过大”。这会间接导致softmax的输出更加平滑，因为大的logit差值才会导致极端的概率分布。 
4. 负熵损失 (Negative Entropy Loss) - 最激进/Router Z-Loss - 中等激进/负载均衡损失 (Load Balancing Loss) - 最不激进